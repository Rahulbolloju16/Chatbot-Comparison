
# ğŸ¤– Chatbot Comparison: Traditional vs Generative AI Models

This project compares the performance and capabilities of **traditional chatbot frameworks** with **modern Generative AI-powered agents** using a unified dataset and evaluation framework.

---

## ğŸ“Œ Objectives

- Benchmark multiple chatbot architectures under consistent input
- Evaluate generative vs rule-based chatbot accuracy, relevance, and speed
- Enable extensibility for future models and datasets

---

## ğŸ“ Project Structure

```
Chatbot-Comparison/
â”œâ”€â”€ bots/                     # All chatbot implementations
â”‚   â”œâ”€â”€ base_bot.py           # Interface all bots inherit from
â”‚   â”œâ”€â”€ openai_bot.py         # OpenAI GPT-based bot
â”‚   â”œâ”€â”€ langchain_bot.py      # LangChain agent bot
â”‚   â”œâ”€â”€ llamaindex_bot.py     # LlamaIndex retrieval-based bot
â”‚   â”œâ”€â”€ haystack_bot.py       # Haystack pipeline bot
â”‚   â”œâ”€â”€ fastchat_bot.py       # FastChat / Vicuna-style LLM bot
â”‚   â”œâ”€â”€ dialogflow_bot.py     # Traditional NLP bot: Dialogflow
â”‚   â”œâ”€â”€ rasa_bot.py           # Traditional NLP bot: Rasa
â”‚   â”œâ”€â”€ botpress_bot.py       # Traditional NLP bot: Botpress
â”‚   â”œâ”€â”€ witai_bot.py          # Traditional NLP bot: Wit.ai
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Knowledge_base/       # Source documents for question answering
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ test_questions.json   # List of test questions to benchmark all bots
â”œâ”€â”€ evaluate.py               # Main script to compare bots
â”œâ”€â”€ requirements.txt          # Required Python packages
```

---

## ğŸ§  Models Compared

### ğŸ”¹ Generative AI Bots

- **OpenAI GPT (via API)**
- **LangChain Agent**
- **LlamaIndex (RAG)**
- **Haystack Pipeline**
- **FastChat (e.g., Vicuna)**

### ğŸ”¸ Traditional Bots

- **Dialogflow**
- **Rasa**
- **Botpress**
- **Wit.ai**

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Rahulbolloju16/Chatbot-Comparison.git
cd Chatbot-Comparison
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Set Up Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your-api-key
```

> Use `python-dotenv` or similar to load these in your code.

### 4. Run Evaluation

```bash
python evaluate.py
```

---

## ğŸ“Š Evaluation Metrics

Each bot is evaluated based on:

- âœ… **Accuracy** (Correctness of answer)
- ğŸ§  **Relevance** (To the source documents)
- â±ï¸ **Response Time**
- ğŸ’¬ **Language Coherence** (for LLMs)

---

## ğŸ“‰ Sample Results Table

| Bot              | Accuracy | Response Time | Notes                      |
|------------------|----------|----------------|-----------------------------|
| OpenAI GPT-4     | 95%      | 1.2s           | High-quality generation     |
| LangChain Agent  | 90%      | 2.3s           | Strong and grounded         |
| Rasa             | 60%      | 0.8s           | Fast, rule-based            |
| Dialogflow       | 65%      | 1.0s           | Good for structured flows   |

---

## ğŸ“¦ Use Cases

- Enterprise Q&A systems
- Helpdesk automation
- GenAI prototype benchmarking
- Research/academic comparisons

---

## ğŸ™‹ Author

**Rahul Bolloju**  
AI/ML & GenAI Engineer  
ğŸ”— [GitHub Profile](https://github.com/Rahulbolloju16)

---

## ğŸ“„ License

This project is licensed under the **MIT License**.  
Use, modify, and contribute freely.

---

## ğŸ”§ Future Enhancements

- [ ] Save evaluation metrics to CSV
- [ ] Add support for vector DBs like FAISS or Pinecone
- [ ] Create a Streamlit UI for interactive testing
- [ ] Include plots and visual comparison charts
